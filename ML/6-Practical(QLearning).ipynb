{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c55e1d-afd6-4f6c-aebf-c0a22c18b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Final Q-table:\n",
      "[[[ 3.97013875  3.97416476  4.01495051  4.845851  ]\n",
      "  [ 4.7378907   4.81161701  4.12058648  5.49539   ]\n",
      "  [ 5.4054968   6.2171      4.42682984  5.43323673]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.0473428   0.08843106 -0.03940399 -0.042661  ]]\n",
      "\n",
      " [[ 0.36199443  5.15332982  1.26723612  0.05544543]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 5.44624324  7.019       5.96735728  6.10584184]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.0527741   3.61845702  0.14230459  0.10177445]]\n",
      "\n",
      " [[ 0.80828579  1.84919549  1.98371271  6.14400125]\n",
      "  [ 4.51401645  1.65500804  2.10403282  7.01715326]\n",
      "  [ 6.1772837   6.98873549  6.07146498  7.91      ]\n",
      "  [ 7.88335439  8.9         6.90454789  6.67875034]\n",
      "  [ 1.13576689  2.69796765  7.84673033  2.99882748]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 1.59325789  3.40098732  0.71131279  0.28977213]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.84601487 10.          8.75442916  8.80519713]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.03258193 -0.02663555  0.06833728  1.46332571]\n",
      "  [ 0.47825841  2.09739054  0.18177722  6.0093313 ]\n",
      "  [ 2.78653188  1.40430184  0.87915386  8.90581011]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#==================================Define the maze layout===========================================\n",
    "maze = np.array([\n",
    "    [0, 0, 0, -1, 0],\n",
    "    [0, -1, 0, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, 0, -1, 0, -1],\n",
    "    [0, 0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=================================Define hyperparameters=============================================\n",
    "alpha = 0.1               # Learning rate\n",
    "gamma = 0.9               # Discount factor\n",
    "epsilon = 0.8             # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#==============================Initialize Q-table with zeros (size of the maze x number of actions)=========================================\n",
    "\n",
    "q_table = np.zeros((maze.shape[0], maze.shape[1], 4)) # 4 possible actions (up, down, left, right)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===========================================Define actions=======================================================================\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================Define function to check if a position is valid========================================\n",
    "\n",
    "def is_valid_position(maze, position):\n",
    "    x, y = position\n",
    "    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=======================================Define function to get next position based on action=====================================\n",
    "\n",
    "def get_next_position(position, action):\n",
    "    x, y = position\n",
    "    dx, dy = actions[action]\n",
    "    next_position = (x + dx, y + dy)\n",
    "    return next_position if is_valid_position(maze, next_position) else position\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================Define reward function===================================================\n",
    "def get_reward(position):\n",
    "    if maze[position] == 1:\n",
    "        return 10                     # Goal reward\n",
    "    elif maze[position] == -1:\n",
    "        return -1                     # Penalty for hitting an obstacle\n",
    "    else:\n",
    "        return -0.1                   # Small penalty for each step taken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================================Q-learning algorithm=================================================================\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    position = (0, 0)  # Start position\n",
    "    done = False\n",
    "\n",
    "\n",
    "    \n",
    "    while not done:\n",
    "        #====================Choose action based on epsilon-greedy policy=============================\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 3)                                    # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[position[0], position[1]])            # Exploit: best action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #=======================================Take action and observe next state and reward====================================\n",
    "        \n",
    "        next_position = get_next_position(position, action)\n",
    "        reward = get_reward(next_position)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #========================================Update Q-value using the Bellman equation========================================\n",
    "        \n",
    "        best_next_action = np.argmax(q_table[next_position[0], next_position[1]])\n",
    "        \n",
    "        td_target = reward + gamma * q_table[next_position[0], next_position[1], best_next_action]\n",
    "        \n",
    "        td_error = td_target - q_table[position[0], position[1], action]\n",
    "        \n",
    "        q_table[position[0], position[1], action] += alpha * td_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #==============================================Update position===============================================\n",
    "        position = next_position\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #===========================================Check if goal is reached============================================\n",
    "        if maze[position] == 1:\n",
    "            done = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #=======================================================Decay epsilon to reduce exploration over time================================\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41188e06-aab1-4689-86a7-17e1ea9c1366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
