{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b81fbc0b-16be-4045-863c-50f32e6b6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Final Q-table:\n",
      "[[[ 4.20004002  4.845851    4.20988726  2.92495978]\n",
      "  [ 0.41131157  0.60617528  4.01036413  0.00275546]\n",
      "  [-0.11092668  0.73190747 -0.21110898 -0.15204965]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.01       -0.01        0.          0.        ]]\n",
      "\n",
      " [[ 4.20681895  5.49539     4.78782664  4.70809623]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.07268262  3.95912858  0.31448369  0.26364998]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.01        0.20083182 -0.0271     -0.01      ]]\n",
      "\n",
      " [[ 4.78174519  5.37461251  5.42253611  6.2171    ]\n",
      "  [ 6.14190844  7.019       5.32983549  6.22752293]\n",
      "  [ 1.15440274  3.17697109  1.97093822  7.58097767]\n",
      "  [ 3.15692188  8.79078181  2.80176111  1.42377799]\n",
      "  [-0.03058243  0.06667159  4.37411318 -0.01      ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 6.0751185   7.91        6.97207931  6.82759245]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 4.60798523  9.97534965  6.21569654  5.49702892]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 2.84538557  3.13709623  1.09578467  7.74365122]\n",
      "  [ 6.89649089  7.77657954  6.31951835  8.9       ]\n",
      "  [ 8.76283416  8.83069082  7.58561389 10.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#================================== Define the maze layout ============================================\n",
    "maze = np.array([\n",
    "    [0, 0, 0, -1, 0],  # 0: free space, -1: obstacle, 1: goal\n",
    "    [0, -1, 0, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, 0, -1, 0, -1],\n",
    "    [0, 0, 0, 1, 0]     # Goal is at (4, 3)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#================================= Define hyperparameters ==============================================\n",
    "alpha = 0.1               # Learning rate\n",
    "gamma = 0.9               # Discount factor\n",
    "epsilon = 0.8             # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 500\n",
    "\n",
    "\n",
    "\n",
    "#=========================================== Define actions ============================================\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#============================== Initialize Q-table with zeros (size of the maze x number of actions) ==========================================\n",
    "no_rows, no_cols = maze.shape\n",
    "q_table = np.zeros((no_rows, no_cols, 4))  # 4 possible actions (up, down, left, right)\n",
    "\n",
    "\n",
    "\n",
    "#======================================== Define function to check if a position is valid =========================================\n",
    "def is_valid_position(maze, position):\n",
    "    x, y = position\n",
    "    return 0 <= x < no_rows and 0 <= y < no_cols and maze[x, y] != -1\n",
    "\n",
    "\n",
    "\n",
    "#======================================= Define function to get next position based on action ======================================\n",
    "def get_next_position(position, action):\n",
    "    x, y = position\n",
    "    dx, dy = actions[action]\n",
    "    next_position = (x + dx, y + dy)\n",
    "    return next_position if is_valid_position(maze, next_position) else position\n",
    "\n",
    "\n",
    "\n",
    "#===================================================== Define reward function ====================================================\n",
    "def get_reward(position):\n",
    "    if maze[position] == 1:\n",
    "        return 10  # Goal reward\n",
    "    elif maze[position] == -1:\n",
    "        return -1  # Penalty for hitting an obstacle\n",
    "    else:\n",
    "        return -0.1  # Small penalty for each step taken\n",
    "\n",
    "\n",
    "\n",
    "#====================================================== Q-learning algorithm ===================================================\n",
    "for episode in range(num_episodes):\n",
    "    position = (0, 0)  # Start position at top-left corner\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        #==================== Choose action based on epsilon-greedy policy =============================\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 3)  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[position[0], position[1]])  # Exploit: best action\n",
    "\n",
    "        \n",
    "        #======================================= Take action and observe next state and reward =====================================\n",
    "        next_position = get_next_position(position, action)\n",
    "        reward = get_reward(next_position)\n",
    "\n",
    "        \n",
    "        #======================================== Update Q-value using Q-learning formula =========================================\n",
    "        q_table[position[0], position[1], action] += alpha * (reward + gamma * np.max(q_table[next_position[0], next_position[1]]) - q_table[position[0], position[1], action])\n",
    "\n",
    "        \n",
    "        #============================================== Update position ================================================\n",
    "        position = next_position\n",
    "\n",
    "        \n",
    "        #=========================================== Check if goal is reached =============================================\n",
    "        if maze[position] == 1:\n",
    "            done = True\n",
    "\n",
    "    \n",
    "    #======================================================= Decay epsilon to reduce exploration over time =================================\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "#======================================================= Display Final Q-table =================================================\n",
    "print(\"Training completed!\")\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7f312-66da-4912-ba48-2c224665cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
